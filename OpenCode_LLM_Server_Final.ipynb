{
    "nbformat": 4,
    "nbformat_minor": 0,
    "metadata": {
        "colab": {
            "provenance": [],
            "gpuType": "T4"
        },
        "kernelspec": {
            "name": "python3",
            "display_name": "Python 3"
        },
        "accelerator": "GPU"
    },
    "cells": [
        {
            "cell_type": "markdown",
            "source": [
                "# ğŸš€ OpenCode LLM Server v6.2\n\n**Based on [ollama-x-opencode](https://github.com/p-lemonish/ollama-x-opencode)**\n\n**Key Fix:** Create a SAVED model with 16K context built-in!\n\n**Recommended:** Use Qwen3 for best tool support"
            ],
            "metadata": {
                "id": "h"
            }
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "c"
            },
            "outputs": [],
            "source": [
                "#@title 1ï¸âƒ£ Choose Model\n# Qwen3 has best tool support according to community feedback\nBASE_MODEL = \"qwen3:8b\"  #@param [\"qwen3:8b\", \"qwen3:4b\", \"llama3.1:8b\", \"mistral:7b\"]\nCONTEXT_SIZE = 16384  #@param [8192, 16384, 32768]\n\n# The model name we'll create with larger context\nMODEL = f\"{BASE_MODEL.split(':')[0]}:{BASE_MODEL.split(':')[1]}-{CONTEXT_SIZE//1024}k\"\nprint(f\"ğŸ“¦ Base: {BASE_MODEL}\")\nprint(f\"ğŸ“¦ Will create: {MODEL} (with {CONTEXT_SIZE} context)\")"
            ]
        },
        {
            "cell_type": "code",
            "source": [
                "#@title 2ï¸âƒ£ Setup\n!nvidia-smi --query-gpu=name,memory.free --format=csv 2>/dev/null || echo 'No GPU'\n!curl -fsSL https://ollama.com/install.sh | sh 2>&1 | tail -1\n!wget -q https://github.com/cloudflare/cloudflared/releases/latest/download/cloudflared-linux-amd64.deb && dpkg -i cloudflared-linux-amd64.deb 2>/dev/null\n!pip install -q flask requests pexpect\nprint('\\nâœ… Ready')"
            ],
            "metadata": {
                "id": "s"
            },
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "code",
            "source": [
                "#@title 3ï¸âƒ£ Start Ollama & Create Model with 16K Context\nimport subprocess, time, os\nimport pexpect\n\nos.environ['OLLAMA_HOST'] = '0.0.0.0:11434'\nos.environ['OLLAMA_ORIGINS'] = '*'\nos.environ['OLLAMA_KEEP_ALIVE'] = '-1'\n\nprint('Starting Ollama...')\nsubprocess.Popen(['ollama', 'serve'], stdout=subprocess.PIPE, stderr=subprocess.PIPE)\ntime.sleep(5)\n\n# Pull base model\nprint(f'\\nDownloading {BASE_MODEL}...')\n!ollama pull {BASE_MODEL}\n\n# Create model with larger context using Modelfile\nprint(f'\\nğŸ“ Creating {MODEL} with {CONTEXT_SIZE} context...')\n\nmodelfile_content = f'''FROM {BASE_MODEL}\nPARAMETER num_ctx {CONTEXT_SIZE}\n'''\n\nwith open('/tmp/Modelfile', 'w') as f:\n    f.write(modelfile_content)\n\n!ollama create {MODEL} -f /tmp/Modelfile\n\nprint(f'\\nâœ… {MODEL} ready with {CONTEXT_SIZE} context!')"
            ],
            "metadata": {
                "id": "o"
            },
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "code",
            "source": [
                "#@title 4ï¸âƒ£ API Server v6.2 - Direct Passthrough\n",
                "from flask import Flask, request, jsonify, Response\n",
                "import requests as req\n",
                "import json, time, uuid\n",
                "\n",
                "app = Flask(__name__)\n",
                "\n",
                "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
                "# DIRECT PASSTHROUGH - Let Ollama handle everything natively\n",
                "# The model now has 16K context built-in, so tools should work!\n",
                "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
                "\n",
                "@app.route('/v1/models', methods=['GET'])\n",
                "def list_models():\n",
                "    return jsonify({'object': 'list', 'data': [{'id': MODEL, 'object': 'model', 'owned_by': 'ollama'}]})\n",
                "\n",
                "@app.route('/v1/chat/completions', methods=['POST'])\n",
                "def chat_completions():\n",
                "    data = request.json or {}\n",
                "    messages = data.get('messages', [])\n",
                "    tools = data.get('tools', [])\n",
                "    \n",
                "    # Get user message for logging\n",
                "    user_msg = ''\n",
                "    for m in reversed(messages or []):\n",
                "        if m.get('role') == 'user':\n",
                "            content = m.get('content', '')\n",
                "            if isinstance(content, list):\n",
                "                for p in content:\n",
                "                    if isinstance(p, dict) and p.get('type') == 'text':\n",
                "                        user_msg = p.get('text', '')[:40]\n",
                "                        break\n",
                "            else:\n",
                "                user_msg = str(content)[:40] if content else ''\n",
                "            break\n",
                "    \n",
                "    tool_names = [t.get('function', {}).get('name', '') for t in tools]\n",
                "    print(f\"[{time.strftime('%H:%M:%S')}] '{user_msg}' | tools={tool_names[:4]}\")\n",
                "    \n",
                "    # Build Ollama request - pass tools directly!\n",
                "    ollama_payload = {\n",
                "        'model': MODEL,\n",
                "        'messages': messages,\n",
                "        'stream': False,\n",
                "    }\n",
                "    \n",
                "    # Pass tools to Ollama exactly as received\n",
                "    if tools:\n",
                "        ollama_payload['tools'] = tools\n",
                "    \n",
                "    try:\n",
                "        result = req.post('http://localhost:11434/api/chat', json=ollama_payload, timeout=180).json()\n",
                "    except Exception as e:\n",
                "        print(f\"  âŒ Ollama error: {e}\")\n",
                "        return jsonify({\n",
                "            'id': f'chatcmpl-{uuid.uuid4().hex[:12]}',\n",
                "            'object': 'chat.completion',\n",
                "            'created': int(time.time()),\n",
                "            'model': MODEL,\n",
                "            'choices': [{'index': 0, 'message': {'role': 'assistant', 'content': 'Error, please try again.'}, 'finish_reason': 'stop'}]\n",
                "        })\n",
                "    \n",
                "    message = result.get('message', {})\n",
                "    content = message.get('content', '')\n",
                "    tool_calls = message.get('tool_calls', [])\n",
                "    \n",
                "    # Build OpenAI-compatible response\n",
                "    response_message = {'role': 'assistant'}\n",
                "    \n",
                "    if tool_calls:\n",
                "        # Model returned tool calls natively!\n",
                "        formatted_calls = []\n",
                "        for i, tc in enumerate(tool_calls):\n",
                "            func = tc.get('function', {})\n",
                "            formatted_calls.append({\n",
                "                'id': f'call_{uuid.uuid4().hex[:8]}',\n",
                "                'type': 'function',\n",
                "                'function': {\n",
                "                    'name': func.get('name', ''),\n",
                "                    'arguments': json.dumps(func.get('arguments', {})) if isinstance(func.get('arguments'), dict) else str(func.get('arguments', '{}'))\n",
                "                }\n",
                "            })\n",
                "        response_message['content'] = None\n",
                "        response_message['tool_calls'] = formatted_calls\n",
                "        finish = 'tool_calls'\n",
                "        print(f\"  â†’ tool: {formatted_calls[0]['function']['name']}\")\n",
                "    else:\n",
                "        # Text response\n",
                "        response_message['content'] = content or \"I'll help you with that!\"\n",
                "        finish = 'stop'\n",
                "        print(f\"  â†’ text: {len(content)} chars\")\n",
                "    \n",
                "    return jsonify({\n",
                "        'id': f'chatcmpl-{uuid.uuid4().hex[:12]}',\n",
                "        'object': 'chat.completion',\n",
                "        'created': int(time.time()),\n",
                "        'model': MODEL,\n",
                "        'system_fingerprint': 'fp_v62',\n",
                "        'choices': [{'index': 0, 'message': response_message, 'logprobs': None, 'finish_reason': finish}],\n",
                "        'usage': {'prompt_tokens': 100, 'completion_tokens': 50, 'total_tokens': 150}\n",
                "    })\n",
                "\n",
                "@app.route('/health', methods=['GET'])\n",
                "def health():\n",
                "    return jsonify({'status': 'ok', 'version': '6.2', 'model': MODEL})\n",
                "\n",
                "@app.route('/', methods=['GET'])\n",
                "def root():\n",
                "    return jsonify({'name': 'OpenCode LLM Server', 'version': '6.2', 'model': MODEL})\n",
                "\n",
                "import threading\n",
                "threading.Thread(target=lambda: app.run(host='0.0.0.0', port=5000, threaded=True, use_reloader=False), daemon=True).start()\n",
                "time.sleep(2)\n",
                "print('\\n' + '='*55)\n",
                "print(f'âœ… Server v6.2 Ready')\n",
                "print(f'   Model: {MODEL}')\n",
                "print(f'   Context: {CONTEXT_SIZE} (built into model!)')\n",
                "print('='*55)"
            ],
            "metadata": {
                "id": "a"
            },
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "code",
            "source": [
                "#@title 5ï¸âƒ£ Test Tool Calling\nimport requests\n\n# Test with OpenCode-like tools\ntest_tools = [\n    {'type': 'function', 'function': {'name': 'write', 'description': 'Write content to a file', 'parameters': {'type': 'object', 'properties': {'filePath': {'type': 'string'}, 'content': {'type': 'string'}}, 'required': ['filePath', 'content']}}},\n    {'type': 'function', 'function': {'name': 'bash', 'description': 'Run a shell command', 'parameters': {'type': 'object', 'properties': {'command': {'type': 'string'}}, 'required': ['command']}}},\n    {'type': 'function', 'function': {'name': 'read', 'description': 'Read a file', 'parameters': {'type': 'object', 'properties': {'filePath': {'type': 'string'}}, 'required': ['filePath']}}},\n]\n\nprint(f'Testing {MODEL}...\\n')\n\nr = requests.post('http://localhost:5000/v1/chat/completions', json={\n    'model': MODEL,\n    'messages': [{'role': 'user', 'content': 'Create a file called hello.py with print(\"hello world\") in it'}],\n    'tools': test_tools\n}, timeout=180)\n\nresult = r.json()\nmsg = result['choices'][0]['message']\n\nif msg.get('tool_calls'):\n    tc = msg['tool_calls'][0]['function']\n    print(f'âœ… SUCCESS! Tool: {tc[\"name\"]}')\n    print(f'   Arguments: {tc[\"arguments\"]}')\nelse:\n    print(f'âŒ Returned text instead of tool:')\n    print(f'   {msg.get(\"content\", \"\")[:200]}')"
            ],
            "metadata": {
                "id": "t"
            },
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "code",
            "source": [
                "#@title 6ï¸âƒ£ Start Cloudflare Tunnel\nimport subprocess, re\nfrom IPython.display import display, HTML\n\nprint('ğŸŒ Starting Cloudflare Tunnel...\\n')\n\ntunnel = subprocess.Popen(\n    ['cloudflared', 'tunnel', '--url', 'http://localhost:5000'],\n    stdout=subprocess.PIPE, stderr=subprocess.STDOUT, text=True\n)\n\nfor line in tunnel.stdout:\n    print(line, end='')\n    if 'trycloudflare.com' in line:\n        match = re.search(r'https://[^\\s]+\\.trycloudflare\\.com', line)\n        if match:\n            url = match.group()\n            display(HTML(f'''\n            <div style=\"background:linear-gradient(135deg,#7c3aed,#a855f7);padding:25px;border-radius:16px;margin:20px 0;color:white;font-family:system-ui\">\n                <h2 style=\"margin:0 0 10px 0\">ğŸš€ v6.2 Ready!</h2>\n                <code style=\"background:rgba(0,0,0,0.2);padding:12px 16px;border-radius:8px;display:block;font-size:16px\">{url}/v1</code>\n                <p style=\"margin:15px 0 0 0;opacity:0.9\">Model: {MODEL} | Context: {CONTEXT_SIZE} (built-in!)</p>\n            </div>\n            <div style=\"background:#fef3c7;color:#92400e;padding:12px 16px;border-radius:10px;font-size:14px\">âš ï¸ Keep this tab open! Update opencode.json with model: \"{MODEL}\"</div>\n            '''))\n            break\n\nprint('\\nğŸ“¡ Tunnel logs:')\nfor line in tunnel.stdout:\n    print(line, end='')"
            ],
            "metadata": {
                "id": "u"
            },
            "execution_count": null,
            "outputs": []
        }
    ]
}