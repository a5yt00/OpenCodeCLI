{
    "nbformat": 4,
    "nbformat_minor": 0,
    "metadata": {
        "colab": {
            "provenance": [],
            "gpuType": "T4"
        },
        "kernelspec": {
            "name": "python3",
            "display_name": "Python 3"
        },
        "accelerator": "GPU"
    },
    "cells": [
        {
            "cell_type": "markdown",
            "source": [
                "# Qwen Coder - Simple & Reliable\\n\\nUses Ollama's native OpenAI-compatible endpoint."
            ],
            "metadata": {
                "id": "intro"
            }
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "setup"
            },
            "outputs": [],
            "source": [
                "!nvidia-smi\n",
                "!curl -fsSL https://ollama.com/install.sh | sh\n",
                "!wget -q https://github.com/cloudflare/cloudflared/releases/latest/download/cloudflared-linux-amd64.deb && dpkg -i cloudflared-linux-amd64.deb\n",
                "print('âœ… Installed')"
            ]
        },
        {
            "cell_type": "code",
            "source": [
                "import subprocess, time, os\n",
                "os.environ['OLLAMA_HOST'] = '0.0.0.0:11434'\n",
                "subprocess.Popen(['ollama', 'serve'], stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
                "time.sleep(5)\n",
                "!ollama pull qwen2.5-coder:7b\n",
                "print('âœ… Model ready')"
            ],
            "metadata": {
                "id": "model"
            },
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "code",
            "source": [
                "# Test Ollama's OpenAI endpoint directly\n",
                "import requests\n",
                "r = requests.post('http://localhost:11434/v1/chat/completions', json={\n",
                "    'model': 'qwen2.5-coder:7b',\n",
                "    'messages': [{'role': 'user', 'content': 'Say hello in one word'}]\n",
                "})\n",
                "print('Status:', r.status_code)\n",
                "print('Response:', r.json())"
            ],
            "metadata": {
                "id": "test"
            },
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "code",
            "source": [
                "# Tunnel directly to Ollama's OpenAI endpoint\n",
                "import subprocess, re\n",
                "from IPython.display import display, HTML\n",
                "\n",
                "tunnel = subprocess.Popen(\n",
                "    ['cloudflared', 'tunnel', '--url', 'http://localhost:11434'],\n",
                "    stdout=subprocess.PIPE, stderr=subprocess.STDOUT, text=True\n",
                ")\n",
                "\n",
                "for line in tunnel.stdout:\n",
                "    print(line, end='')\n",
                "    if 'trycloudflare.com' in line:\n",
                "        m = re.search(r'https://[^\\s]+\\.trycloudflare\\.com', line)\n",
                "        if m:\n",
                "            url = m.group()\n",
                "            display(HTML(f'''<div style=\"background:linear-gradient(135deg,#667eea,#764ba2);padding:20px;border-radius:10px\">\n",
                "            <h2 style=\"color:white\">ðŸŽ‰ Ready!</h2>\n",
                "            <code style=\"color:white;font-size:16px\">{url}/v1</code>\n",
                "            <p style=\"color:#eee;margin-top:10px\">OpenCode: /connect â†’ Other â†’ Base URL: {url}/v1</p>\n",
                "            </div>'''))\n",
                "            print(f'\\n\\nðŸ“‹ Copy this: {url}/v1')\n",
                "            break\n",
                "\n",
                "for line in tunnel.stdout:\n",
                "    print(line, end='')"
            ],
            "metadata": {
                "id": "tunnel"
            },
            "execution_count": null,
            "outputs": []
        }
    ]
}