{
    "nbformat": 4,
    "nbformat_minor": 0,
    "metadata": {
        "colab": {
            "provenance": [],
            "gpuType": "T4"
        },
        "kernelspec": {
            "name": "python3",
            "display_name": "Python 3"
        },
        "accelerator": "GPU"
    },
    "cells": [
        {
            "cell_type": "markdown",
            "source": [
                "# Qwen Coder - Simple Working Version\n\nThis version focuses on **reliability** - always returns a response."
            ],
            "metadata": {
                "id": "intro"
            }
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "setup"
            },
            "outputs": [],
            "source": [
                "!nvidia-smi\n",
                "!curl -fsSL https://ollama.com/install.sh | sh\n",
                "!wget -q https://github.com/cloudflare/cloudflared/releases/latest/download/cloudflared-linux-amd64.deb && dpkg -i cloudflared-linux-amd64.deb\n",
                "!pip install -q flask requests\n",
                "print('✅ Setup done')"
            ]
        },
        {
            "cell_type": "code",
            "source": [
                "import subprocess, time, os\n",
                "os.environ['OLLAMA_HOST'] = '0.0.0.0:11434'\n",
                "os.environ['OLLAMA_ORIGINS'] = '*'\n",
                "subprocess.Popen(['ollama', 'serve'], stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
                "time.sleep(5)\n",
                "!ollama pull qwen2.5-coder:7b\n",
                "print('\\n✅ Model ready')"
            ],
            "metadata": {
                "id": "ollama"
            },
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "code",
            "source": [
                "# SIMPLE RELIABLE API - Ignores tools, always returns content\n",
                "from flask import Flask, request, jsonify\n",
                "import requests as req\n",
                "import json, time, uuid, threading\n",
                "\n",
                "app = Flask(__name__)\n",
                "\n",
                "@app.route('/v1/models', methods=['GET'])\n",
                "def models():\n",
                "    return jsonify({\"object\": \"list\", \"data\": [{\"id\": \"qwen2.5-coder:7b\", \"object\": \"model\", \"owned_by\": \"ollama\"}]})\n",
                "\n",
                "@app.route('/v1/chat/completions', methods=['POST'])\n",
                "def chat():\n",
                "    data = request.json\n",
                "    messages = data.get('messages', [])\n",
                "    \n",
                "    # Clean messages - only keep role and content\n",
                "    clean_messages = []\n",
                "    for m in messages:\n",
                "        role = m.get('role', 'user')\n",
                "        content = m.get('content', '')\n",
                "        if role == 'tool':\n",
                "            role = 'user'\n",
                "            content = f\"Tool result: {content}\"\n",
                "        if content:  # Only add if there's content\n",
                "            clean_messages.append({'role': role, 'content': str(content)})\n",
                "    \n",
                "    if not clean_messages:\n",
                "        clean_messages = [{'role': 'user', 'content': 'Hello'}]\n",
                "    \n",
                "    # Call Ollama - simple, no tools\n",
                "    content = \"I'm ready to help with your coding task!\"\n",
                "    try:\n",
                "        print(f\"Calling Ollama with {len(clean_messages)} messages...\")\n",
                "        r = req.post('http://localhost:11434/api/chat', json={\n",
                "            'model': 'qwen2.5-coder:7b',\n",
                "            'messages': clean_messages,\n",
                "            'stream': False,\n",
                "            'options': {'num_ctx': 8192, 'temperature': 0.7}\n",
                "        }, timeout=120)\n",
                "        \n",
                "        if r.status_code == 200:\n",
                "            result = r.json()\n",
                "            content = result.get('message', {}).get('content', '')\n",
                "            print(f\"Got response: {content[:100]}...\")\n",
                "        else:\n",
                "            print(f\"Ollama error: {r.status_code}\")\n",
                "            content = \"I encountered an issue but I'm here to help. Please try again.\"\n",
                "            \n",
                "    except Exception as e:\n",
                "        print(f\"Exception: {e}\")\n",
                "        content = f\"I'm having trouble connecting. Please try again.\"\n",
                "    \n",
                "    # GUARANTEE content is never empty\n",
                "    if not content or not content.strip():\n",
                "        content = \"I understand. How can I help you with your code?\"\n",
                "    \n",
                "    response = {\n",
                "        \"id\": f\"chatcmpl-{uuid.uuid4().hex[:12]}\",\n",
                "        \"object\": \"chat.completion\",\n",
                "        \"created\": int(time.time()),\n",
                "        \"model\": \"qwen2.5-coder:7b\",\n",
                "        \"choices\": [{\n",
                "            \"index\": 0,\n",
                "            \"message\": {\"role\": \"assistant\", \"content\": content},\n",
                "            \"finish_reason\": \"stop\"\n",
                "        }],\n",
                "        \"usage\": {\"prompt_tokens\": 50, \"completion_tokens\": len(content.split()), \"total_tokens\": 50 + len(content.split())}\n",
                "    }\n",
                "    print(f\"Returning: {json.dumps(response)[:200]}...\")\n",
                "    return jsonify(response)\n",
                "\n",
                "@app.route('/health', methods=['GET'])\n",
                "def health():\n",
                "    return jsonify({\"status\": \"ok\"})\n",
                "\n",
                "threading.Thread(target=lambda: app.run(host='0.0.0.0', port=5000, threaded=True, use_reloader=False), daemon=True).start()\n",
                "time.sleep(3)\n",
                "print('\\n✅ Simple API running on port 5000')"
            ],
            "metadata": {
                "id": "api"
            },
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "code",
            "source": [
                "# Quick test\n",
                "import requests\n",
                "r = requests.post('http://localhost:5000/v1/chat/completions', json={\n",
                "    'model': 'qwen2.5-coder:7b',\n",
                "    'messages': [{'role': 'user', 'content': 'Write hello world in Python'}]\n",
                "})\n",
                "print(\"Response:\", r.json()['choices'][0]['message']['content'][:300])"
            ],
            "metadata": {
                "id": "test"
            },
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "code",
            "source": [
                "# Start Tunnel\n",
                "import subprocess, re\n",
                "from IPython.display import display, HTML\n",
                "\n",
                "tunnel = subprocess.Popen(['cloudflared', 'tunnel', '--url', 'http://localhost:5000'],\n",
                "    stdout=subprocess.PIPE, stderr=subprocess.STDOUT, text=True)\n",
                "\n",
                "for line in tunnel.stdout:\n",
                "    print(line, end='')\n",
                "    if 'trycloudflare.com' in line:\n",
                "        m = re.search(r'https://[^\\s]+\\.trycloudflare\\.com', line)\n",
                "        if m:\n",
                "            url = m.group()\n",
                "            display(HTML(f'''<div style=\"background:#2ecc71;padding:25px;border-radius:15px\">\n",
                "            <h2 style=\"color:white\">✅ API Ready!</h2>\n",
                "            <p style=\"color:white;font-size:20px;font-family:monospace\">{url}/v1</p>\n",
                "            </div>'''))\n",
                "            break\n",
                "\n",
                "for line in tunnel.stdout:\n",
                "    print(line, end='')"
            ],
            "metadata": {
                "id": "tunnel"
            },
            "execution_count": null,
            "outputs": []
        }
    ]
}