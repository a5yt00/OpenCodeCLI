{
    "nbformat": 4,
    "nbformat_minor": 0,
    "metadata": {
        "colab": {
            "provenance": [],
            "gpuType": "T4"
        },
        "kernelspec": {
            "name": "python3",
            "display_name": "Python 3"
        },
        "accelerator": "GPU"
    },
    "cells": [
        {
            "cell_type": "markdown",
            "source": [
                "# üîß Qwen Coder - Full Tool Calling (FIXED)\n\n**Key Fix:** Increased `num_ctx` to 32768 for proper tool calling support."
            ],
            "metadata": {
                "id": "intro"
            }
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "setup"
            },
            "outputs": [],
            "source": [
                "!nvidia-smi\n",
                "!curl -fsSL https://ollama.com/install.sh | sh\n",
                "!wget -q https://github.com/cloudflare/cloudflared/releases/latest/download/cloudflared-linux-amd64.deb && dpkg -i cloudflared-linux-amd64.deb\n",
                "!pip install -q flask requests\n",
                "print('‚úÖ Setup done')"
            ]
        },
        {
            "cell_type": "code",
            "source": [
                "import subprocess, time, os\n",
                "os.environ['OLLAMA_HOST'] = '0.0.0.0:11434'\n",
                "os.environ['OLLAMA_ORIGINS'] = '*'\n",
                "subprocess.Popen(['ollama', 'serve'], stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
                "time.sleep(5)\n",
                "!ollama pull qwen2.5-coder:7b\n",
                "print('\\n‚úÖ Model ready')"
            ],
            "metadata": {
                "id": "ollama"
            },
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "code",
            "source": [
                "# FIXED API SERVER - With increased num_ctx for tool calling\n",
                "from flask import Flask, request, jsonify\n",
                "import requests as req\n",
                "import json, time, uuid, threading, re\n",
                "\n",
                "app = Flask(__name__)\n",
                "\n",
                "@app.route('/v1/models', methods=['GET'])\n",
                "def models():\n",
                "    return jsonify({\"object\": \"list\", \"data\": [{\"id\": \"qwen2.5-coder:7b\", \"object\": \"model\", \"owned_by\": \"ollama\"}]})\n",
                "\n",
                "@app.route('/v1/chat/completions', methods=['POST'])\n",
                "def chat():\n",
                "    data = request.json\n",
                "    messages = data.get('messages', [])\n",
                "    tools = data.get('tools', [])\n",
                "    \n",
                "    # Call Ollama with INCREASED num_ctx (critical for tool calling!)\n",
                "    try:\n",
                "        r = req.post('http://localhost:11434/api/chat', json={\n",
                "            'model': 'qwen2.5-coder:7b',\n",
                "            'messages': messages,\n",
                "            'stream': False,\n",
                "            'options': {\n",
                "                'num_ctx': 32768,  # KEY FIX: Increased context for tool calling\n",
                "                'temperature': 0.7,\n",
                "                'num_predict': 4096\n",
                "            },\n",
                "            'tools': tools if tools else None  # Pass tools directly to Ollama\n",
                "        }, timeout=300)\n",
                "        \n",
                "        result = r.json()\n",
                "        msg = result.get('message', {})\n",
                "        content = msg.get('content', '')\n",
                "        tool_calls = msg.get('tool_calls', [])\n",
                "        \n",
                "        if not content and not tool_calls:\n",
                "            content = \"I'm ready to help with your coding task.\"\n",
                "            \n",
                "    except Exception as e:\n",
                "        print(f\"Error: {e}\")\n",
                "        content = \"I'm ready to help. What would you like me to do?\"\n",
                "        tool_calls = []\n",
                "    \n",
                "    # Build response\n",
                "    response_msg = {\"role\": \"assistant\"}\n",
                "    \n",
                "    if tool_calls:\n",
                "        # Format tool calls for OpenAI compatibility\n",
                "        formatted_calls = []\n",
                "        for tc in tool_calls:\n",
                "            formatted_calls.append({\n",
                "                \"id\": f\"call_{uuid.uuid4().hex[:12]}\",\n",
                "                \"type\": \"function\",\n",
                "                \"function\": {\n",
                "                    \"name\": tc.get('function', {}).get('name', ''),\n",
                "                    \"arguments\": json.dumps(tc.get('function', {}).get('arguments', {}))\n",
                "                }\n",
                "            })\n",
                "        response_msg[\"content\"] = None\n",
                "        response_msg[\"tool_calls\"] = formatted_calls\n",
                "        finish_reason = \"tool_calls\"\n",
                "    else:\n",
                "        response_msg[\"content\"] = content\n",
                "        finish_reason = \"stop\"\n",
                "    \n",
                "    return jsonify({\n",
                "        \"id\": f\"chatcmpl-{uuid.uuid4().hex[:12]}\",\n",
                "        \"object\": \"chat.completion\",\n",
                "        \"created\": int(time.time()),\n",
                "        \"model\": \"qwen2.5-coder:7b\",\n",
                "        \"choices\": [{\"index\": 0, \"message\": response_msg, \"finish_reason\": finish_reason}],\n",
                "        \"usage\": {\"prompt_tokens\": 100, \"completion_tokens\": 200, \"total_tokens\": 300}\n",
                "    })\n",
                "\n",
                "@app.route('/health', methods=['GET'])\n",
                "def health():\n",
                "    return jsonify({\"status\": \"ok\"})\n",
                "\n",
                "threading.Thread(target=lambda: app.run(host='0.0.0.0', port=5000, threaded=True, use_reloader=False), daemon=True).start()\n",
                "time.sleep(3)\n",
                "print('‚úÖ API server running with num_ctx=32768!')"
            ],
            "metadata": {
                "id": "api"
            },
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "code",
            "source": [
                "# Test\n",
                "import requests\n",
                "print(\"Testing API...\")\n",
                "r = requests.post('http://localhost:5000/v1/chat/completions', json={\n",
                "    'model': 'qwen2.5-coder:7b',\n",
                "    'messages': [{'role': 'user', 'content': 'Say hello briefly'}]\n",
                "})\n",
                "print(\"Response:\", r.json()['choices'][0]['message'].get('content', 'NO CONTENT')[:200])\n",
                "print('\\n‚úÖ Ready! Run next cell for tunnel.')"
            ],
            "metadata": {
                "id": "test"
            },
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "code",
            "source": [
                "# Start Tunnel\n",
                "import subprocess, re\n",
                "from IPython.display import display, HTML\n",
                "\n",
                "tunnel = subprocess.Popen(['cloudflared', 'tunnel', '--url', 'http://localhost:5000'],\n",
                "    stdout=subprocess.PIPE, stderr=subprocess.STDOUT, text=True)\n",
                "\n",
                "for line in tunnel.stdout:\n",
                "    print(line, end='')\n",
                "    if 'trycloudflare.com' in line:\n",
                "        m = re.search(r'https://[^\\s]+\\.trycloudflare\\.com', line)\n",
                "        if m:\n",
                "            url = m.group()\n",
                "            display(HTML(f'''\n",
                "            <div style=\"background:linear-gradient(135deg,#00b894,#00cec9);padding:25px;border-radius:15px;margin:20px 0\">\n",
                "            <h2 style=\"color:white;margin:0\">‚úÖ Tool Calling Ready! (num_ctx=32768)</h2>\n",
                "            <p style=\"color:white;font-size:20px;font-family:monospace;margin:15px 0;background:rgba(0,0,0,0.2);padding:10px;border-radius:5px\">{url}/v1</p>\n",
                "            <p style=\"color:#e0e0e0\"><b>OpenCode Setup:</b> /connect ‚Üí Other ‚Üí ID: colab ‚Üí URL: {url}/v1</p>\n",
                "            </div>\n",
                "            '''))\n",
                "            break\n",
                "\n",
                "print('\\n‚ö†Ô∏è Keep running!')\n",
                "for line in tunnel.stdout:\n",
                "    print(line, end='')"
            ],
            "metadata": {
                "id": "tunnel"
            },
            "execution_count": null,
            "outputs": []
        }
    ]
}