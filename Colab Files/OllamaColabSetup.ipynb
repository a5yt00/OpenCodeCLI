{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# üöÄ Qwen Coder on Google Colab with Ollama\n",
        "\n",
        "This notebook sets up **Qwen2.5-Coder** using **Ollama** and exposes the API via **Cloudflare Tunnel** for use with OpenCode.\n",
        "\n",
        "## Quick Start\n",
        "1. Make sure GPU runtime is enabled (Runtime ‚Üí Change runtime type ‚Üí T4 GPU)\n",
        "2. Run all cells in order\n",
        "3. Copy the `trycloudflare.com` URL from the output\n",
        "4. Use the URL in OpenCode as your API endpoint\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "intro"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1Ô∏è‚É£ Check GPU Availability"
      ],
      "metadata": {
        "id": "gpu_header"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "check_gpu"
      },
      "outputs": [],
      "source": [
        "# Check if GPU is available\n",
        "!nvidia-smi\n",
        "\n",
        "import torch\n",
        "if torch.cuda.is_available():\n",
        "    gpu_name = torch.cuda.get_device_name(0)\n",
        "    gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1e9\n",
        "    print(f\"\\n‚úÖ GPU Available: {gpu_name}\")\n",
        "    print(f\"üìä VRAM: {gpu_memory:.1f} GB\")\n",
        "else:\n",
        "    print(\"‚ùå No GPU detected! Please enable GPU in Runtime settings.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2Ô∏è‚É£ Install Ollama"
      ],
      "metadata": {
        "id": "ollama_header"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install Ollama\n",
        "!curl -fsSL https://ollama.com/install.sh | sh\n",
        "print(\"\\n‚úÖ Ollama installed successfully!\")"
      ],
      "metadata": {
        "id": "install_ollama"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3Ô∏è‚É£ Install Cloudflared (for tunneling)"
      ],
      "metadata": {
        "id": "cloudflare_header"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install Cloudflared\n",
        "!wget -q https://github.com/cloudflare/cloudflared/releases/latest/download/cloudflared-linux-amd64.deb\n",
        "!dpkg -i cloudflared-linux-amd64.deb\n",
        "!cloudflared --version\n",
        "print(\"\\n‚úÖ Cloudflared installed successfully!\")"
      ],
      "metadata": {
        "id": "install_cloudflared"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4Ô∏è‚É£ Start Ollama Server"
      ],
      "metadata": {
        "id": "server_header"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import subprocess\n",
        "import time\n",
        "import os\n",
        "\n",
        "# Set environment variable to allow all origins (for API access)\n",
        "os.environ['OLLAMA_HOST'] = '0.0.0.0:11434'\n",
        "os.environ['OLLAMA_ORIGINS'] = '*'\n",
        "\n",
        "# Start Ollama server in background\n",
        "ollama_process = subprocess.Popen(\n",
        "    ['ollama', 'serve'],\n",
        "    stdout=subprocess.PIPE,\n",
        "    stderr=subprocess.PIPE,\n",
        "    env=os.environ\n",
        ")\n",
        "\n",
        "# Wait for server to start\n",
        "time.sleep(5)\n",
        "\n",
        "# Check if server is running\n",
        "import requests\n",
        "try:\n",
        "    response = requests.get('http://localhost:11434/api/tags', timeout=5)\n",
        "    if response.status_code == 200:\n",
        "        print(\"‚úÖ Ollama server is running on port 11434!\")\n",
        "    else:\n",
        "        print(f\"‚ö†Ô∏è Server responded with status: {response.status_code}\")\n",
        "except:\n",
        "    print(\"‚è≥ Server starting... please wait a moment and try again.\")"
      ],
      "metadata": {
        "id": "start_server"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5Ô∏è‚É£ Download Qwen Coder Model\n",
        "\n",
        "**Available models for T4 GPU (15GB VRAM):**\n",
        "- `qwen2.5-coder:7b` - Recommended for free tier (fast)\n",
        "- `qwen2.5-coder:3b` - Lighter option\n",
        "- `qwen2.5-coder:1.5b` - Fastest, smallest\n",
        "\n",
        "For larger models, use the AirLLM notebook instead."
      ],
      "metadata": {
        "id": "model_header"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Pull the Qwen Coder model (this may take a few minutes)\n",
        "# Change the model name below if you want a different size\n",
        "\n",
        "MODEL_NAME = \"qwen2.5-coder:7b\"  # @param [\"qwen2.5-coder:7b\", \"qwen2.5-coder:3b\", \"qwen2.5-coder:1.5b\"]\n",
        "\n",
        "print(f\"üì• Downloading {MODEL_NAME}...\")\n",
        "print(\"This may take 5-10 minutes depending on model size.\\n\")\n",
        "\n",
        "!ollama pull {MODEL_NAME}\n",
        "\n",
        "print(f\"\\n‚úÖ Model {MODEL_NAME} downloaded successfully!\")"
      ],
      "metadata": {
        "id": "pull_model"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6Ô∏è‚É£ Test the Model Locally"
      ],
      "metadata": {
        "id": "test_header"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import json\n",
        "\n",
        "# Test with a simple prompt\n",
        "response = requests.post(\n",
        "    'http://localhost:11434/api/generate',\n",
        "    json={\n",
        "        'model': MODEL_NAME,\n",
        "        'prompt': 'Write a Python function to calculate factorial',\n",
        "        'stream': False\n",
        "    },\n",
        "    timeout=120\n",
        ")\n",
        "\n",
        "if response.status_code == 200:\n",
        "    result = response.json()\n",
        "    print(\"‚úÖ Model test successful!\\n\")\n",
        "    print(\"Response:\")\n",
        "    print(\"-\" * 50)\n",
        "    print(result.get('response', 'No response')[:500])\n",
        "else:\n",
        "    print(f\"‚ùå Test failed with status: {response.status_code}\")\n",
        "    print(response.text)"
      ],
      "metadata": {
        "id": "test_model"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 7Ô∏è‚É£ Start Cloudflare Tunnel üåê\n",
        "\n",
        "This will create a public URL for your Ollama API.\n",
        "\n",
        "**‚ö†Ô∏è Keep this cell running!** The tunnel stays active as long as this cell is executing."
      ],
      "metadata": {
        "id": "tunnel_header"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import subprocess\n",
        "import re\n",
        "import time\n",
        "from IPython.display import display, HTML\n",
        "\n",
        "print(\"üöÄ Starting Cloudflare Tunnel...\\n\")\n",
        "\n",
        "# Start cloudflared tunnel\n",
        "tunnel = subprocess.Popen(\n",
        "    ['cloudflared', 'tunnel', '--url', 'http://localhost:11434'],\n",
        "    stdout=subprocess.PIPE,\n",
        "    stderr=subprocess.STDOUT,\n",
        "    text=True\n",
        ")\n",
        "\n",
        "# Extract the public URL\n",
        "public_url = None\n",
        "for line in tunnel.stdout:\n",
        "    print(line, end='')\n",
        "    if 'trycloudflare.com' in line:\n",
        "        match = re.search(r'https://[^\\s]+\\.trycloudflare\\.com', line)\n",
        "        if match:\n",
        "            public_url = match.group()\n",
        "            break\n",
        "\n",
        "if public_url:\n",
        "    # Display the URL prominently\n",
        "    display(HTML(f'''\n",
        "    <div style=\"background: linear-gradient(135deg, #667eea 0%, #764ba2 100%); \n",
        "                padding: 20px; border-radius: 10px; margin: 20px 0;\">\n",
        "        <h2 style=\"color: white; margin: 0;\">üéâ Your API is Live!</h2>\n",
        "        <p style=\"color: #f0f0f0; font-size: 14px;\">Use this URL in OpenCode or any API client:</p>\n",
        "        <div style=\"background: rgba(255,255,255,0.2); padding: 10px; border-radius: 5px; \n",
        "                    font-family: monospace; font-size: 16px; color: white;\">\n",
        "            {public_url}\n",
        "        </div>\n",
        "        <br>\n",
        "        <p style=\"color: #f0f0f0; font-size: 12px; margin: 0;\">\n",
        "            <b>OpenCode Config:</b><br>\n",
        "            ‚Ä¢ Provider: Ollama<br>\n",
        "            ‚Ä¢ Base URL: {public_url}<br>\n",
        "            ‚Ä¢ Model: {MODEL_NAME}\n",
        "        </p>\n",
        "    </div>\n",
        "    '''))\n",
        "    \n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"üìã COPY THIS URL FOR OPENCODE:\")\n",
        "    print(f\"   {public_url}\")\n",
        "    print(\"=\"*60)\n",
        "    print(f\"\\nüîß Model: {MODEL_NAME}\")\n",
        "    print(\"\\n‚ö†Ô∏è Keep this cell running! The tunnel closes when you stop it.\")\n",
        "    print(\"\\n\" + \"-\"*60)\n",
        "    print(\"Tunnel logs:\")\n",
        "    \n",
        "    # Keep reading output to keep tunnel alive\n",
        "    for line in tunnel.stdout:\n",
        "        print(line, end='')"
      ],
      "metadata": {
        "id": "start_tunnel"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "## üìñ API Usage Examples\n",
        "\n",
        "### Generate Text (Ollama Native API)\n",
        "```bash\n",
        "curl YOUR_URL/api/generate -d '{\n",
        "  \"model\": \"qwen2.5-coder:7b\",\n",
        "  \"prompt\": \"Write a Python hello world\",\n",
        "  \"stream\": false\n",
        "}'\n",
        "```\n",
        "\n",
        "### Chat (Ollama Native API)\n",
        "```bash\n",
        "curl YOUR_URL/api/chat -d '{\n",
        "  \"model\": \"qwen2.5-coder:7b\",\n",
        "  \"messages\": [{\"role\": \"user\", \"content\": \"Hello!\"}],\n",
        "  \"stream\": false\n",
        "}'\n",
        "```\n",
        "\n",
        "### OpenAI-Compatible (v1 API)\n",
        "```bash\n",
        "curl YOUR_URL/v1/chat/completions -d '{\n",
        "  \"model\": \"qwen2.5-coder:7b\",\n",
        "  \"messages\": [{\"role\": \"user\", \"content\": \"Hello!\"}]\n",
        "}'\n",
        "```"
      ],
      "metadata": {
        "id": "usage_examples"
      }
    }
  ]
}
