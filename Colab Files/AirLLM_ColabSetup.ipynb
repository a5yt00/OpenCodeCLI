{
    "nbformat": 4,
    "nbformat_minor": 0,
    "metadata": {
        "colab": {
            "provenance": [],
            "gpuType": "T4"
        },
        "kernelspec": {
            "name": "python3",
            "display_name": "Python 3"
        },
        "language_info": {
            "name": "python"
        },
        "accelerator": "GPU"
    },
    "cells": [
        {
            "cell_type": "markdown",
            "source": [
                "# üß† Qwen 32B on Colab with AirLLM\n\nRun **Qwen2.5-Coder-32B** on limited VRAM using **AirLLM's layer-by-layer inference**.\n\n> ‚ö†Ô∏è GLM-4.7 not supported (requires newer transformers incompatible with AirLLM)\n\n---"
            ],
            "metadata": {
                "id": "intro"
            }
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "setup"
            },
            "outputs": [],
            "source": [
                "#@title 1Ô∏è‚É£ Setup (~2 min)\n",
                "!nvidia-smi --query-gpu=name,memory.free --format=csv 2>/dev/null || echo 'No GPU'\n",
                "\n",
                "# Pinned versions for AirLLM compatibility\n",
                "!pip install -q \"optimum<2.0\" \"transformers<4.49\" 2>&1 | tail -1\n",
                "!pip install -q airllm accelerate bitsandbytes sentencepiece flask einops 2>&1 | tail -1\n",
                "!wget -q https://github.com/cloudflare/cloudflared/releases/latest/download/cloudflared-linux-amd64.deb && dpkg -i cloudflared-linux-amd64.deb 2>/dev/null\n",
                "\n",
                "import torch\n",
                "print(f\"\\n‚úÖ Ready! GPU: {torch.cuda.get_device_name(0) if torch.cuda.is_available() else 'None'}\")"
            ]
        },
        {
            "cell_type": "code",
            "source": [
                "#@title 2Ô∏è‚É£ Select Model\n",
                "import re\n",
                "\n",
                "# Qwen models work great with AirLLM\n",
                "PRESET_MODEL = \"Qwen/Qwen2.5-Coder-32B-Instruct\"  #@param [\"Qwen/Qwen2.5-Coder-32B-Instruct\", \"Qwen/Qwen2.5-Coder-14B-Instruct\", \"Qwen/Qwen2.5-Coder-7B-Instruct\", \"Qwen/Qwen2.5-72B-Instruct\", \"THUDM/chatglm3-6b\", \"Custom\"]\n",
                "CUSTOM_MODEL = \"\"  #@param {type:\"string\"}\n",
                "COMPRESSION = \"4bit\"  #@param [\"4bit\", \"8bit\", \"none\"]\n",
                "\n",
                "def parse_hf_url(url):\n",
                "    match = re.match(r'https?://(?:huggingface|hf)\\.co/([^/]+/[^/]+)', url.strip())\n",
                "    return match.group(1) if match else url.strip()\n",
                "\n",
                "MODEL_ID = parse_hf_url(CUSTOM_MODEL) if CUSTOM_MODEL.strip() else PRESET_MODEL\n",
                "MODEL_NAME = MODEL_ID.split('/')[-1].lower().replace('-', '_').replace('.', '_')\n",
                "print(f\"üì¶ Model: {MODEL_ID} | Compression: {COMPRESSION}\")"
            ],
            "metadata": {
                "id": "select"
            },
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "code",
            "source": [
                "#@title 3Ô∏è‚É£ Load Model with AirLLM (‚è≥ 15-30 min first time)\n",
                "from airllm import AutoModel\n",
                "import gc, torch\n",
                "\n",
                "gc.collect(); torch.cuda.empty_cache()\n",
                "print(f\"üîÑ Loading {MODEL_ID}...\")\n",
                "print(\"‚è≥ First download: 15-30 min for 32B model\\n\")\n",
                "\n",
                "model = AutoModel.from_pretrained(MODEL_ID, compression=COMPRESSION if COMPRESSION != 'none' else None)\n",
                "print(f\"\\n‚úÖ Loaded: {MODEL_ID}\")"
            ],
            "metadata": {
                "id": "load"
            },
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "code",
            "source": [
                "#@title 4Ô∏è‚É£ Test Generation\n",
                "prompt = \"Write a Python function to check if a number is prime:\"\n",
                "print(f\"Testing: {prompt[:50]}...\\n\")\n",
                "\n",
                "try:\n",
                "    formatted = model.tokenizer.apply_chat_template([{\"role\": \"user\", \"content\": prompt}], tokenize=False, add_generation_prompt=True)\n",
                "except:\n",
                "    formatted = f\"User: {prompt}\\nAssistant:\"\n",
                "\n",
                "ids = model.tokenizer(formatted, return_tensors=\"pt\").input_ids.to('cuda')\n",
                "out = model.generate(ids, max_new_tokens=256, do_sample=True, temperature=0.7)\n",
                "print(model.tokenizer.decode(out[0], skip_special_tokens=True))"
            ],
            "metadata": {
                "id": "test"
            },
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "code",
            "source": [
                "#@title 5Ô∏è‚É£ Start API Server\n",
                "from flask import Flask, request, jsonify\n",
                "import threading, time, uuid\n",
                "\n",
                "app = Flask(__name__)\n",
                "\n",
                "@app.route('/v1/models', methods=['GET'])\n",
                "def models_list(): return jsonify({'object': 'list', 'data': [{'id': MODEL_NAME, 'object': 'model'}]})\n",
                "\n",
                "@app.route('/v1/chat/completions', methods=['POST'])\n",
                "def chat():\n",
                "    data = request.json\n",
                "    msgs = data.get('messages', [])\n",
                "    try:\n",
                "        fmt = model.tokenizer.apply_chat_template(msgs, tokenize=False, add_generation_prompt=True)\n",
                "    except:\n",
                "        fmt = \"\\n\".join([f\"{m['role'].title()}: {m['content']}\" for m in msgs]) + \"\\nAssistant:\"\n",
                "    ids = model.tokenizer(fmt, return_tensors=\"pt\").input_ids.to('cuda')\n",
                "    out = model.generate(ids, max_new_tokens=min(data.get('max_tokens', 512), 2048), do_sample=True, temperature=data.get('temperature', 0.7))\n",
                "    txt = model.tokenizer.decode(out[0][len(ids[0]):], skip_special_tokens=True)\n",
                "    return jsonify({'id': f'chatcmpl-{uuid.uuid4().hex[:8]}', 'object': 'chat.completion', 'created': int(time.time()), 'model': MODEL_NAME, 'choices': [{'index': 0, 'message': {'role': 'assistant', 'content': txt.strip()}, 'finish_reason': 'stop'}]})\n",
                "\n",
                "@app.route('/health', methods=['GET'])\n",
                "def health(): return jsonify({'status': 'ok', 'model': MODEL_NAME})\n",
                "\n",
                "threading.Thread(target=lambda: app.run(host='0.0.0.0', port=5000, threaded=True), daemon=True).start()\n",
                "time.sleep(2)\n",
                "print(f\"‚úÖ API Ready: /v1/chat/completions\")"
            ],
            "metadata": {
                "id": "api"
            },
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "code",
            "source": [
                "#@title 6Ô∏è‚É£ Start Tunnel üåê (Keep Running!)\n",
                "import subprocess, re\n",
                "from IPython.display import display, HTML\n",
                "\n",
                "print(\"üöÄ Starting tunnel...\\n\")\n",
                "tunnel = subprocess.Popen(['cloudflared', 'tunnel', '--url', 'http://localhost:5000'], stdout=subprocess.PIPE, stderr=subprocess.STDOUT, text=True)\n",
                "\n",
                "for line in tunnel.stdout:\n",
                "    if 'trycloudflare.com' in line:\n",
                "        url = re.search(r'https://[^\\s]+\\.trycloudflare\\.com', line).group()\n",
                "        display(HTML(f'<div style=\"background:#10b981;padding:15px;border-radius:10px;color:white\"><h3>üéâ API Live!</h3><code>{url}/v1</code><br><small>Model: {MODEL_NAME}</small></div>'))\n",
                "        print(f\"\\nüìã OpenCode Config:\")\n",
                "        print(f\"   Base URL: {url}/v1\")\n",
                "        print(f\"   Model: {MODEL_NAME}\")\n",
                "        print(f\"   API Key: sk-dummy\")\n",
                "        break\n",
                "\n",
                "for line in tunnel.stdout: print(line, end='')"
            ],
            "metadata": {
                "id": "tunnel"
            },
            "execution_count": null,
            "outputs": []
        }
    ]
}