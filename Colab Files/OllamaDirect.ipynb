{
    "nbformat": 4,
    "nbformat_minor": 0,
    "metadata": {
        "colab": {
            "provenance": [],
            "gpuType": "T4"
        },
        "kernelspec": {
            "name": "python3",
            "display_name": "Python 3"
        },
        "accelerator": "GPU"
    },
    "cells": [
        {
            "cell_type": "markdown",
            "source": [
                "# Ollama Direct - No Custom Code\n\nThis uses Ollama's **native** OpenAI-compatible endpoint directly."
            ],
            "metadata": {
                "id": "intro"
            }
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "setup"
            },
            "outputs": [],
            "source": [
                "!nvidia-smi\n",
                "!curl -fsSL https://ollama.com/install.sh | sh\n",
                "!wget -q https://github.com/cloudflare/cloudflared/releases/latest/download/cloudflared-linux-amd64.deb && dpkg -i cloudflared-linux-amd64.deb\n",
                "print('‚úÖ Installed')"
            ]
        },
        {
            "cell_type": "code",
            "source": [
                "import subprocess, time, os\n",
                "os.environ['OLLAMA_HOST'] = '0.0.0.0:11434'\n",
                "os.environ['OLLAMA_ORIGINS'] = '*'\n",
                "subprocess.Popen(['ollama', 'serve'], stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
                "time.sleep(5)\n",
                "!ollama pull qwen2.5-coder:7b\n",
                "print('\\n‚úÖ Ollama ready with qwen2.5-coder:7b')"
            ],
            "metadata": {
                "id": "ollama"
            },
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "code",
            "source": [
                "# Test Ollama's native OpenAI endpoint\n",
                "import requests\n",
                "r = requests.post('http://localhost:11434/v1/chat/completions', json={\n",
                "    'model': 'qwen2.5-coder:7b',\n",
                "    'messages': [{'role': 'user', 'content': 'Say hello'}]\n",
                "})\n",
                "print(\"Status:\", r.status_code)\n",
                "print(\"Response:\", r.json())\n",
                "if r.status_code == 200:\n",
                "    print(\"\\n‚úÖ Ollama OpenAI API working!\")"
            ],
            "metadata": {
                "id": "test"
            },
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "code",
            "source": [
                "# Tunnel directly to Ollama (port 11434)\n",
                "import subprocess, re\n",
                "from IPython.display import display, HTML\n",
                "\n",
                "print(\"Starting tunnel to Ollama's native API...\")\n",
                "tunnel = subprocess.Popen(['cloudflared', 'tunnel', '--url', 'http://localhost:11434'],\n",
                "    stdout=subprocess.PIPE, stderr=subprocess.STDOUT, text=True)\n",
                "\n",
                "for line in tunnel.stdout:\n",
                "    print(line, end='')\n",
                "    if 'trycloudflare.com' in line:\n",
                "        m = re.search(r'https://[^\\s]+\\.trycloudflare\\.com', line)\n",
                "        if m:\n",
                "            url = m.group()\n",
                "            display(HTML(f'''<div style=\"background:#9b59b6;padding:25px;border-radius:15px\">\n",
                "            <h2 style=\"color:white\">üéØ Ollama Native API</h2>\n",
                "            <p style=\"color:white;font-size:20px;font-family:monospace\">{url}/v1</p>\n",
                "            <hr style=\"border-color:rgba(255,255,255,0.3)\">\n",
                "            <p style=\"color:#eee\">This is Ollama's built-in OpenAI endpoint - no custom code!</p>\n",
                "            </div>'''))\n",
                "            break\n",
                "\n",
                "print('\\n‚ö†Ô∏è Keep running!')\n",
                "for line in tunnel.stdout:\n",
                "    print(line, end='')"
            ],
            "metadata": {
                "id": "tunnel"
            },
            "execution_count": null,
            "outputs": []
        }
    ]
}